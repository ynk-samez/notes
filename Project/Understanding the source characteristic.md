#ResearchProject #Sorces
## Gradients flow is smooth
- [[1906.04285v2.pdf|Continuous time Analysis of Momentum Methods]]
	- written by Nikola B.Kovachki, Andrew M.Stuart
	- published in 2020
	- *What is new*
		- They clarified that it is possible to approximate the gradient flow as an  Ordinary Differential Equation(ODE) in general
		- They proved the above statement.
	- *Introduction*
		- machine learning tasks are optimization problem 
		- $\Phi: \mathbb{R}^d\rightarrow \mathbb{R}$ *loss function*
			- differentiable
			- non-convex
		- formalised as: 
			- $$
		\arg \underset{u\in \mathbb{R}^d}{\min}\Phi(u)
		 $$
		- *Stocastic gradient method*
			- SGD (Stocastic gradient decent)
				$$
w\lefta
		 $$
			 - *HB  :  Heavy Ball Method*
				 - 
			 - *NAG  :  Nesterovâ€™s method of accelerated gradients*
			