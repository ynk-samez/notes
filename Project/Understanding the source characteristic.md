#ResearchProject #Sorces
## Gradients flow is smooth
- [[1906.04285v2.pdf|Continuous time Analysis of Momentum Methods]]
	- written by Nikola B.Kovachki, Andrew M.Stuart
	- published in 2020
	- *What is new*
		- They clarified that it is possible to approximate the gradient flow as an  Ordinary Differential Equation(ODE) in general
		- They proved the above statement.
	- *Introduction*
		- machine learning tasks are optimization problem 
		- $\Phi: \mathbb{R}^d\rightarrow \mathbb{R}$ *loss function*
			- differentiable
			- non-convex
		- formalised as: 
			- $$
		\arg \underset{u\in \mathbb{R}^d}{\min}\Phi(u)
		 $$
		- *Stocastic gradient method*
			- SGD (Stocastic gradient decent)
				$$
w\leftarrow w-\eta\frac{\partial E}{\partial w}
		 $$
			- **Improovement for SGD** 
				-  *Momentum*
					- 
				 - *HB  :  Heavy Ball Method*
				 - 
				 - *NAG  :  Nesterov’s method of accelerated gradients*
		- to the best of my knowledge
		- indispensability of 


1. Quantize:
	1. 2nd Differential
	2. continuos acceralation
	3. 

![[Pasted image 20260114141852.png]]
A. 3. b 
B. 4. a (d)
C. 1. c
D. 2. d


## 実践編
- 環境との相互作用
	- 共振
	- 
